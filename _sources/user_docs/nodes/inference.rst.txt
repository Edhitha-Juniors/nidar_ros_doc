YOLO Inference Node
===================

The **YoloInferenceNode** provides a complete real-time aerial object detection
and geolocation pipeline for autonomous drone missions.

It consumes geotagged image metadata, performs inference using a YOLOv8 model,
filters detections, estimates the real-world GPS coordinates of detected objects,
and publishes structured outputs for downstream mission planning and target
management.

Overview
--------

Autonomous scouting and surveillance missions require more than object detection.
Detected objects must be mapped into the real world with geographic coordinates.

The **YoloInferenceNode** bridges this gap by combining:

- Geotagged drone state (GPS + yaw)
- High-performance YOLO inference
- Pixel-to-world coordinate projection
- Persistent logging and cropped evidence storage

This enables drones to autonomously detect and localize targets during flight.

Core Responsibilities
---------------------

- Subscribe to geotagged image metadata from the Geotagger node
- Load captured image frames from disk
- Run YOLOv8 inference on each incoming image
- Filter detections by:

  - Target class whitelist
  - Confidence threshold
  - Bounding box size constraints

- Apply Non-Max Suppression (NMS) to remove duplicate detections
- Compute estimated GPS coordinates of each detection using Ground Sample Distance (GSD)
- Save outputs including:

  - Cropped detection images
  - Annotated processed frames
  - Master detection CSV log

- Publish each valid detection as a JSON message for mission-level target tracking

Input Workflow
--------------

The node operates downstream of the camera + geotagging pipeline:

1. Image captured by `ImageCaptureNode`
2. Geotag metadata generated by `Geotagger`
3. Inference triggered on `/geotag_data`

Expected geotag message format:

::

   timestamp,lat,lon,alt,orientation_w,yaw,filename

Example:

::

   1770208123.45,12.9716,77.5946,914.2,0.98,135.4,image00023.jpg

Detection Output Format
-----------------------

Each valid detection is published as a JSON payload:

::

   {
     "Image Name": "image00023.jpg",
     "Crop Name": "image00023_crop0.jpg",
     "Latitude": 12.97172,
     "Longitude": 77.59481,
     "Class": "person",
     "Confidence": 0.87,
     "Timestamp": "1770208123.45",
     "Yaw": 135.4
   }

These messages are consumed by higher-level modules such as:

- `cluster`
- `target_manager`
- `cluster_delivery`

ROS Interfaces
--------------

Subscribed Topics
~~~~~~~~~~~~~~~~~

- `/geotag_data` *(std_msgs/String)*  
  Provides image timestamp, drone GPS location, yaw, and filename.

Published Topics
~~~~~~~~~~~~~~~~

- `/yolo_results_topic` *(std_msgs/String)*  
  Publishes one JSON detection message per valid object.

- `/inference/complete` *(std_msgs/String)*  
  Signals completion of inference for a specific image frame.

Detection Filtering
-------------------

The node applies several validation layers to ensure only meaningful targets are
reported:

- Class filtering using a predefined whitelist (`TARGET_CLASSES`)
- Confidence threshold (`conf_threshold`)
- Bounding box area ratio constraints:

  - Rejects overly large boxes (`max_area_ratio`)
  - Rejects overly small boxes (`min_area_ratio`)

Non-Max Suppression
-------------------

When multiple overlapping detections occur, the node applies IoU-based NMS:

- Configurable via `IOU_THRESHOLD`

This ensures only the strongest detection remains per target instance.

Geolocation Projection
----------------------

For each detection, the pixel offset between:

- Image center (drone nadir)
- Bounding box center (target)

is converted into a real-world displacement using:

- Ground Sample Distance (`gsd`)

The displacement is rotated by the drone yaw and projected into latitude/longitude
space, producing an estimated target coordinate.

Filesystem Outputs
------------------

The node generates persistent artifacts for post-flight analysis:

- Cropped detection evidence:

  ::

     <folder>/crops/*.jpg

- Annotated processed images:

  ::

     processed_<image>.jpg

- Master CSV log:

  ::

     inference.csv

Parameters
----------

- `folder_path` *(string)*  
  Base directory containing captured frames and inference outputs.

- `yolo_model_path` *(string)*  
  Path to YOLOv8 model weights (`.pt`).

- `conf_threshold` *(float)*  
  Minimum detection confidence.

- `max_area_ratio` *(float)*  
  Maximum bounding box area fraction.

- `min_area_ratio` *(float)*  
  Minimum bounding box area fraction.

- `IOU_THRESHOLD` *(float)*  
  IoU threshold for Non-Max Suppression.

- `gsd` *(float)*  
  Ground Sample Distance (cm/pixel), used for coordinate projection.

Typical Use Case
----------------

The YOLO Inference Node is deployed in missions requiring:

- Autonomous surveillance
- Target detection and localization
- Evidence logging for delivery or rescue workflows
- Multi-drone target clustering and assignment

It forms the perception backbone of the Drone Scout stack by transforming raw
images into actionable geolocated targets.
